# Metin analizi Uygulaması 

install.packages("ROAuth")        
install.packages("twitteR")      
install.packages("tm")           
install.packages("wordcloud")     
install.packages("ggplot2")       
install.packages("RColorBrewed")  
install.packages("stringr")      
install.packages("dplyr")         
install.packages("tidytext")     
install.packages("readr")        


# Kullandığımız paketleri aktifleştirelim.

library(ROAuth)
library(twitteR)
library(tm)
library(wordcloud)
library(ggplot2)
library(Rcolor)
library(stringr)
library(dplyr)
library(tidytext)
library(readr)
library(stringi)


#API Bilgilerimiz

api_key <- "rDtVJJjeunXFCJtHNMR3FSvSO"
api_secret_key <- "aGgdtzZiKeNISIryN0kOKuivkDoFCRa500TKEVCxfdv52FPMIl"
access_token <- "1296063762-pbiaTUhlcu4QSlWd2iebkwp7REqna8ZIj5YefJ6"
access_token_secret <- "yGiVhCVXJfB8dAaphWFtHDRB5sftv6qm26dlBSZ4OD8OK"

setup_twitter_oauth(api_key,api_secret_key,access_token,access_token_secret)


# Twitter "Netflix" Resmi web sayfası incelenmiştir ve 21 günlük veriler çekilmiştir.

netflixData <- searchTwitteR('@netflix', n = 12000, lang="en")  


#Length methodu kaç tane çektiğimizi gösterir.

save(netflixData, file="netflixData.RData") #Data Kaydedilir.
length(netflixData)

head(netflixData)

#myCorpus adında bir corpus oluşturalım.

netflixData.text <- sapply(netflixData, function(x) x$getText())
mycorpus <- Corpus(VectorSource(netflixData.text))




# Tekrar eden verileri yüklediğimiz paketle kaldırma işlemini gerçekleştiriyoruz.

rmv_word <- c(stopwords("en"))

#Listeledik.
rmv_word




# Veri Temizleme İşlemi


removeURL <- function(x) gsub("http[[:alnum:]]*","",x)
removeNumPuncts <- function(x) gsub("[^[:alpha:][:space:]]*","",x)


tw_corp <- tm_map(mycorpus, PlainTextDocument)
tw_corp <- tm_map(tw_corp, content_transformer(removeURL))
tw_corp <- tm_map(tw_corp, stripWhitespace)
tw_corp <- tm_map(tw_corp, content_transformer(tolower))
tw_corp <- tm_map(tw_corp, removeWords, rmv_word)
tw_corp <- tm_map(tw_corp, content_transformer(removeNumPuncts))
tw_corp <- tm_map(tw_corp, removePunctuation)

netflixData.text[10] #Verinin temizlenmemiş hali.



tw_corp[[10]][1] #Verinin temizlenmiş hali.




save(tw_corp, file="netflixData.csv") #Data Kaydedilir.
length(netflixData)

tweets.df <- twListToDF(tw_corp)  

write.csv(netflixData.text, file="tweetsnetflix.csv", row.names=FALSE)  


#Terim Dökümantasyon Değerleri
netflixData_tdm <- TermDocumentMatrix(tw_corp)
netflixData_tdm


#matrix oluşturma
netflixData_m <- as.matrix(netflixData_tdm)
dim(netflixData_m)


#Frequency Kullanılan kelimelerin sayılarının azalan sıraya göre sıralamak.
term_freq <- rowSums(netflixData_m)
term_freq <- sort(term_freq, decreasing = TRUE)

term_freq[1:10] #10 tane veriyi getirdik.


#Grafik halinde görelim
barplot(term_freq[1:10], col="tan", las=2)


#Frekansı 800 ve daha üzeri olan kelimelerin grafiği
term_freq <- subset(term_freq, term_freq >=800)
term_freqs_df <- data.frame(term= names(term_freq), freq= term_freq)
ggplot(term_freqs_df, aes(x=term, y=freq)) + 
  geom_line(aes(group=1), colour="salmon3")+
  geom_point(size=3, colour="pink2") + xlab("kelime") + ylab("Frekans") + coord_flip()


#Kelime Bulutu Oluşturma

wordcloud(tw_corp, min.freq = 2, scale =c(2,0.5),colors = brewer.pal(8,"Dark2"),
          random.color = TRUE, random.order = FALSE, max.words = 200)


#Frekansa göre kelime bulutu oluşturma
word_freqs <- data.frame(term_freq, term = names(term_freq), num= term_freq)
wordcloud(word_freqs$term, word_freqs$num, max.words = 200, colors=brewer.pal(11,"RdYlGn"),random.color = TRUE)


#İlişkilendirmeler

#Netflix'i  enjoy="zevk, keyif" ilgili ilişkilendirdik.
#Korelasyon limiti en az %50 olanları getirdik.
#Ve ilişkili kelimeler, "typing","peek","almost" gibi kelimeler gelmiştir.


findAssocs(netflixData_tdm, terms="enjoy",corlimit = 0.5)

































































